apiVersion: apps/v1
kind: Deployment
metadata:
  name: compile
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: compile
  template:
    metadata:
      labels:
        app: compile
    spec:
      schedulerName: my-scheduler
      nodeSelector:
        karpenter.sh/nodepool: neuron-trn1-efa
      containers:
        - name: app
          image: public.ecr.aws/neuron/pytorch-inference-vllm-neuronx:0.7.2-neuronx-py310-sdk2.24.0-ubuntu22.04 #<---update latest vllm-inference-neuronx registry
          imagePullPolicy: Always
          command:
            - /bin/bash
            - "-exc"
            - |
              set -x
              apt-get update
              export NEURON_COMPILED_ARTIFACTS="/$COMPILED_MODEL_ID"
              python3 -m vllm.entrypoints.openai.api_server \
                --model "deepseek-ai/DeepSeek-R1-Distill-Llama-70B" \
                --tensor-parallel-size 32 \
                --max-num-seqs 8 \
                --max-model-len 16384 \
                --block-size 8 \
                --device neuron \
                --use-v2-block-manager \
                --port 8000
          resources:
            requests:
              aws.amazon.com/neuron: 16
            limits:
              aws.amazon.com/neuron: 16
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
          env:
            - name: VLLM_NEURON_FRAMEWORK
              value: "neuronx-distributed-inference"
            - name: COMPILED_MODEL_ID
              value: "yahavb/DeepSeek-R1-Distill-Llama-70B-BS8-SL16k-TP32"
            - name: HUGGINGFACE_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-secrets
                  key: HUGGINGFACE_TOKEN
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
